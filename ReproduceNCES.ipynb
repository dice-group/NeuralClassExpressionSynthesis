{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impossible-destruction",
   "metadata": {},
   "source": [
    "## Evaluate NCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "improving-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from utils.syntax_checker import SyntaxChecker\n",
    "from utils.evaluator import Evaluator\n",
    "from ontolearn.knowledge_base import KnowledgeBase\n",
    "from nces import BaseConceptSynthesis\n",
    "from nces.synthesizer import ConceptSynthesizer\n",
    "from utils.data import Data\n",
    "from owlapy.parser import DLSyntaxParser\n",
    "from dataloader import CSDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "killing-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import json\n",
    "import torch, pandas as pd\n",
    "with open(\"settings.json\") as setting:\n",
    "    args = json.load(setting)\n",
    "args = Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gentle-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vietnamese-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_pad(arg):\n",
    "    arg_temp = []\n",
    "    for atm in arg:\n",
    "        if atm == 'PAD':\n",
    "            break\n",
    "        arg_temp.append(atm)\n",
    "    return arg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "academic-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(prediction, target):\n",
    "    def soft(arg1, arg2):\n",
    "        arg1_ = arg1\n",
    "        arg2_ = arg2\n",
    "        if isinstance(arg1_, str):\n",
    "            arg1_ = set(before_pad(BaseConceptSynthesis.decompose(arg1_)))\n",
    "        else:\n",
    "            arg1_ = set(before_pad(arg1_))\n",
    "        if isinstance(arg2_, str):\n",
    "            arg2_ = set(before_pad(BaseConceptSynthesis.decompose(arg2_)))\n",
    "        else:\n",
    "            arg2_ = set(before_pad(arg2_))\n",
    "        return 100*float(len(arg1_.intersection(arg2_)))/len(arg1_.union(arg2_))\n",
    "\n",
    "    def hard(arg1, arg2):\n",
    "        arg1_ = arg1\n",
    "        arg2_ = arg2\n",
    "        if isinstance(arg1_, str):\n",
    "            arg1_ = before_pad(BaseConceptSynthesis.decompose(arg1_))\n",
    "        else:\n",
    "            arg1_ = before_pad(arg1_)\n",
    "        if isinstance(arg2_, str):\n",
    "            arg2_ = before_pad(BaseConceptSynthesis.decompose(arg2_))\n",
    "        else:\n",
    "            arg2_ = before_pad(arg2_)\n",
    "        return 100*float(sum(map(lambda x,y: x==y, arg1_, arg2_)))/max(len(arg1_), len(arg2_))\n",
    "    soft_acc = sum(map(soft, prediction, target))/len(target)\n",
    "    hard_acc = sum(map(hard, prediction, target))/len(target)\n",
    "    return soft_acc, hard_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flexible-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_examples(pos, neg, num_examples):\n",
    "    if min(len(neg),len(pos)) >= num_examples//2:\n",
    "        if len(pos) > len(neg):\n",
    "            num_neg_ex = num_examples//2\n",
    "            num_pos_ex = num_examples-num_neg_ex\n",
    "        else:\n",
    "            num_pos_ex = num_examples//2\n",
    "            num_neg_ex = num_examples-num_pos_ex\n",
    "    elif len(pos) > len(neg):\n",
    "        num_neg_ex = len(neg)\n",
    "        num_pos_ex = num_examples-num_neg_ex\n",
    "    elif len(pos) < len(neg):\n",
    "        num_pos_ex = len(pos)\n",
    "        num_neg_ex = num_examples-num_pos_ex\n",
    "    positive = random.sample(pos, num_pos_ex)\n",
    "    negative = random.sample(neg, num_neg_ex)\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "literary-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_token(model, idx_array):\n",
    "    return model.inv_vocab[idx_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extended-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    pos_emb_list = []\n",
    "    neg_emb_list = []\n",
    "    target_tokens_list = []\n",
    "    target_labels = []\n",
    "    for pos_emb, neg_emb, label in batch:\n",
    "        pos_emb_list.append(pos_emb)\n",
    "        neg_emb_list.append(neg_emb)\n",
    "        target_labels.append(label)\n",
    "    pos_emb_list = pad_sequence(pos_emb_list, batch_first=True, padding_value=0)\n",
    "    neg_emb_list = pad_sequence(neg_emb_list, batch_first=True, padding_value=0)\n",
    "    target_labels = pad_sequence(target_labels, batch_first=True, padding_value=-100)\n",
    "    return pos_emb_list, neg_emb_list, target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "green-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(kb, embeddings, kwargs):\n",
    "    data_test_path = f\"datasets/{kb}/Test_data/Data.json\"\n",
    "    with open(data_test_path, \"r\") as file:\n",
    "        data_test = json.load(file)\n",
    "    data_test = list(data_test.items())\n",
    "    test_dataset = CSDataLoader(data_test, embeddings, kwargs)\n",
    "    print(\"Number of learning problems: \", len(test_dataset))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=kwargs.batch_size, num_workers=kwargs.num_workers, collate_fn=collate_batch, shuffle=False)\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "false-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_prediction(models, x1, x2):\n",
    "    for i,model in enumerate(models):\n",
    "        model.eval()\n",
    "        if i == 0:\n",
    "            _, scores = model(x1, x2)\n",
    "        else:\n",
    "            _, sc = model(x1, x2)\n",
    "            scores = scores + sc\n",
    "    scores = scores/len(models)\n",
    "    prediction = model.inv_vocab[scores.argmax(1)]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "becoming-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_expressions(model_name, kb, args, ensemble=False):\n",
    "    args.knowledge_base_path = \"datasets/\"+f\"{kb}/{kb}.owl\"\n",
    "    embeddings = pd.read_csv(f\"embeddings/{kb}/ConEx_entity_embeddings.csv\").set_index('Unnamed: 0')\n",
    "    dataloader = get_data(kb, embeddings, args)\n",
    "    if ensemble:\n",
    "        models = [torch.load(f\"datasets/{kb}/Model_weights/{name}.pt\", map_location=torch.device('cpu'))\\\n",
    "                  for name in [\"SetTransformer\", \"GRU\", \"LSTM\"]]\n",
    "    if not ensemble:\n",
    "        model = torch.load(f\"datasets/{kb}/Model_weights/{model_name}.pt\", map_location=torch.device('cpu'))\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = models[0]\n",
    "        model.eval()\n",
    "    soft_acc, hard_acc = 0.0, 0.0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for x1, x2, labels in tqdm(dataloader):\n",
    "        target_sequence = map_to_token(model, labels)\n",
    "        if ensemble:\n",
    "            pred_sequence = get_ensemble_prediction(models, x1, x2)\n",
    "        else:\n",
    "            pred_sequence, _ = model(x1, x2)\n",
    "        preds.append(pred_sequence)\n",
    "        targets.append(target_sequence)\n",
    "        s_acc, h_acc = compute_accuracy(pred_sequence, target_sequence)\n",
    "        soft_acc += s_acc\n",
    "        hard_acc += h_acc\n",
    "    print(f\"Average syntactic accuracy, Soft: {soft_acc/len(dataloader)}%, Hard: {hard_acc/len(dataloader)}%\")\n",
    "    return np.concatenate(preds, 0), np.concatenate(targets, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "latest-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(kb_name, args, verbose=False):\n",
    "    print('#'*50)\n",
    "    print('NCES evaluation on {} KB:'.format(kb_name))\n",
    "    print('#'*50)\n",
    "    All_metrics = {\"Ensemble\": defaultdict(lambda: defaultdict(list))}\n",
    "    print()\n",
    "    kb = KnowledgeBase(path=f\"datasets/{kb_name}/{kb_name}.owl\")\n",
    "    namespace = kb.ontology()._onto.base_iri\n",
    "    if kb_name == 'family-benchmark':\n",
    "        namespace = 'http://www.benchmark.org/family#'\n",
    "    print(\"KB namespace: \", namespace)\n",
    "    print()\n",
    "    syntax_checker = SyntaxChecker(kb)\n",
    "    evaluator = Evaluator(kb)\n",
    "    dl_parser = DLSyntaxParser(namespace = namespace)\n",
    "    All_individuals = set(kb.individuals())\n",
    "    with open(f\"datasets/{kb_name}/Test_data/Data.json\", \"r\") as file:\n",
    "        data_test = json.load(file)\n",
    "        \n",
    "    t0 = time.time()\n",
    "    predictions, targets = predict_class_expressions(\"Ensemble\", kb_name, args, ensemble=True)\n",
    "    t1 = time.time()\n",
    "    duration = (t1-t0)/len(predictions)\n",
    "    print()\n",
    "    print(f\"## Ensemble ##\")\n",
    "    print()\n",
    "    for i, pb_str in enumerate(targets):\n",
    "        pb_str = \"\".join(before_pad(pb_str))\n",
    "        #examples = data_test[pb_str]\n",
    "        #pos_examples = set(examples['positive examples'])\n",
    "        #neg_examples = set(examples['negative examples'])\n",
    "        try:\n",
    "            end_idx = np.where(predictions[i] == 'PAD')[0][0] # remove padding token\n",
    "        except IndexError:\n",
    "            end_idx = 1\n",
    "        pred = predictions[i][:end_idx]\n",
    "        #print(\"Before parsing: \", pred.sum())\n",
    "        succeed = False\n",
    "        if (pred=='(').sum() > (pred==')').sum():\n",
    "            for i in range(len(pred))[::-1]:\n",
    "                try:\n",
    "                    prediction = dl_parser.parse_expression(\"\".join(pred.tolist().insert(i,')')))\n",
    "                    succeed = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not succeed:\n",
    "                try:\n",
    "                    pred = syntax_checker.correct(pred.sum())\n",
    "                    pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                    prediction = syntax_checker.get_concept(pred)\n",
    "                except Exception:\n",
    "                    print(f\"Could not understand expression {pred}\")\n",
    "                    continue\n",
    "        elif (pred==')').sum() > (pred=='(').sum():\n",
    "            for i in range(len(pred)):\n",
    "                try:\n",
    "                    prediction = dl_parser.parse_expression(\"\".join(pred.tolist().insert(i,'(')))\n",
    "                    succeed = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not succeed:\n",
    "                try:\n",
    "                    pred = syntax_checker.correct(pred.sum())\n",
    "                    pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                    prediction = syntax_checker.get_concept(pred)\n",
    "                except Exception:\n",
    "                    print(f\"Could not understand expression {pred}\")\n",
    "                    continue\n",
    "        else:\n",
    "            try:\n",
    "                prediction = dl_parser.parse_expression(\"\".join(pred.tolist()))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    pred = syntax_checker.correct(pred.sum())\n",
    "                    pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                    prediction = syntax_checker.get_concept(pred)\n",
    "                except Exception:\n",
    "                    print(f\"Could not understand expression {pred}\")\n",
    "                    continue\n",
    "        target_expression = dl_parser.parse_expression(pb_str) # The target class expression\n",
    "        try:\n",
    "            positive_examples = {ind.get_iri().as_str().split(\"/\")[-1] for ind in kb.individuals(target_expression)}\n",
    "            negative_examples = All_individuals-positive_examples\n",
    "            acc, f1 = evaluator.evaluate(prediction, positive_examples, negative_examples)\n",
    "        except NotImplementedError:\n",
    "            print(\"Invalid target or predicted expression, skipping\")\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f'Problem {i}, Target: {pb_str}, Prediction: {syntax_checker.renderer.render(prediction)}, Acc: {acc}, F1: {f1}')\n",
    "            print()\n",
    "        All_metrics[\"Ensemble\"]['acc']['values'].append(acc)\n",
    "        All_metrics[\"Ensemble\"]['prediction']['values'].append(syntax_checker.renderer.render(prediction))\n",
    "        All_metrics[\"Ensemble\"]['f1']['values'].append(f1)\n",
    "        All_metrics[\"Ensemble\"]['time']['values'].append(duration)\n",
    "\n",
    "    for metric in All_metrics[\"Ensemble\"]:\n",
    "        if metric != 'prediction':\n",
    "            All_metrics[\"Ensemble\"][metric]['mean'] = [np.mean(All_metrics[\"Ensemble\"][metric]['values'])]\n",
    "            All_metrics[\"Ensemble\"][metric]['std'] = [np.std(All_metrics[\"Ensemble\"][metric]['values'])]\n",
    "\n",
    "    print(\"Ensemble\"+' Speed: {}s +- {} / lp'.format(round(All_metrics[\"Ensemble\"]['time']['mean'][0], 2),\\\n",
    "                                                           round(All_metrics[\"Ensemble\"]['time']['std'][0], 2)))\n",
    "    print(\"Ensemble\"+' Avg Acc: {}% +- {} / lp'.format(round(All_metrics[\"Ensemble\"]['acc']['mean'][0], 2),\\\n",
    "                                                           round(All_metrics[\"Ensemble\"]['acc']['std'][0], 2)))\n",
    "    print(\"Ensemble\"+' Avg F1: {}% +- {} / lp'.format(round(All_metrics[\"Ensemble\"]['f1']['mean'][0], 2),\\\n",
    "                                                           round(All_metrics[\"Ensemble\"]['f1']['std'][0], 2)))\n",
    "#        print(\"Ensemble\"+' Avg Str_Acc: {}% +- {} / lp'.format(round(All_metrics[\"Ensemble\"]['str_acc']['mean'][0], 2),\\\n",
    "#                                                               round(All_metrics[\"Ensemble\"]['str_acc']['std'][0], 2)))\n",
    "    \n",
    "    print()\n",
    "\n",
    "    with open(\"datasets/\"+kb_name+\"/Results/NCES_Ensemble.json\", \"w\") as file:\n",
    "        json.dump(All_metrics, file, indent=3, ensure_ascii=False)\n",
    "    return All_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "introductory-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nces(kb_name, models, args, verbose=False):\n",
    "    print('#'*50)\n",
    "    print('NCES evaluation on {} KB:'.format(kb_name))\n",
    "    print('#'*50)\n",
    "    desc = \"\"\n",
    "    if args.shuffle_examples:\n",
    "        desc = \"_shuffle\"\n",
    "    All_metrics = {m: defaultdict(lambda: defaultdict(list)) for m in models}\n",
    "    print()\n",
    "    kb = KnowledgeBase(path=f\"datasets/{kb_name}/{kb_name}.owl\")\n",
    "    namespace = kb.ontology()._onto.base_iri\n",
    "    if kb_name == 'family-benchmark':\n",
    "        namespace = 'http://www.benchmark.org/family#'\n",
    "    print(\"KB namespace: \", namespace)\n",
    "    print()\n",
    "    syntax_checker = SyntaxChecker(kb)\n",
    "    evaluator = Evaluator(kb)\n",
    "    dl_parser = DLSyntaxParser(namespace = namespace)\n",
    "    All_individuals = set(kb.individuals())\n",
    "    with open(f\"datasets/{kb_name}/Test_data/Data.json\", \"r\") as file:\n",
    "        data_test = json.load(file)\n",
    "    for model_name in models:\n",
    "        t0 = time.time()\n",
    "        predictions, targets = predict_class_expressions(model_name, kb_name, args)\n",
    "        t1 = time.time()\n",
    "        duration = (t1-t0)/len(predictions)\n",
    "        print()\n",
    "        print(f\"##{model_name}##\")\n",
    "        print()\n",
    "        for i, pb_str in enumerate(targets):\n",
    "            pb_str = \"\".join(before_pad(pb_str))\n",
    "            #examples = data_test[pb_str]\n",
    "            #pos_examples = set(examples['positive examples'])\n",
    "            #neg_examples = set(examples['negative examples'])\n",
    "            try:\n",
    "                end_idx = np.where(predictions[i] == 'PAD')[0][0] # remove padding token\n",
    "            except IndexError:\n",
    "                end_idx = 1\n",
    "            pred = predictions[i][:end_idx]\n",
    "            #print(\"Before parsing: \", pred.sum())\n",
    "            succeed = False\n",
    "            if (pred=='(').sum() > (pred==')').sum():\n",
    "                for i in range(len(pred))[::-1]:\n",
    "                    try:\n",
    "                        prediction = dl_parser.parse_expression(\"\".join(pred.tolist().insert(i,')')))\n",
    "                        succeed = True\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if not succeed:\n",
    "                    try:\n",
    "                        pred = syntax_checker.correct(pred.sum())\n",
    "                        pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                        prediction = syntax_checker.get_concept(pred)\n",
    "                    except Exception:\n",
    "                        print(f\"Could not understand expression {pred}\")\n",
    "                        continue\n",
    "            elif (pred==')').sum() > (pred=='(').sum():\n",
    "                for i in range(len(pred)):\n",
    "                    try:\n",
    "                        prediction = dl_parser.parse_expression(\"\".join(pred.tolist().insert(i,'(')))\n",
    "                        succeed = True\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if not succeed:\n",
    "                    try:\n",
    "                        pred = syntax_checker.correct(pred.sum())\n",
    "                        pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                        prediction = syntax_checker.get_concept(pred)\n",
    "                    except Exception:\n",
    "                        print(f\"Could not understand expression {pred}\")\n",
    "                        continue\n",
    "            else:\n",
    "                try:\n",
    "                    prediction = dl_parser.parse_expression(\"\".join(pred.tolist()))\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        pred = syntax_checker.correct(pred.sum())\n",
    "                        pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                        prediction = syntax_checker.get_concept(pred)\n",
    "                    except Exception:\n",
    "                        print(f\"Could not understand expression {pred}\")\n",
    "                        continue\n",
    "            target_expression = dl_parser.parse_expression(pb_str) # The target class expression\n",
    "            try:\n",
    "                positive_examples = {ind.get_iri().as_str().split(\"/\")[-1] for ind in kb.individuals(target_expression)}\n",
    "                negative_examples = All_individuals-positive_examples\n",
    "                acc, f1 = evaluator.evaluate(prediction, positive_examples, negative_examples)\n",
    "            except NotImplementedError:\n",
    "                print(\"Invalid target or predicted expression, skipping\")\n",
    "                continue\n",
    "            if verbose:\n",
    "                print(f'Problem {i}, Target: {pb_str}, Prediction: {syntax_checker.renderer.render(prediction)}, Acc: {acc}, F1: {f1}')\n",
    "                print()\n",
    "            All_metrics[model_name]['acc']['values'].append(acc)\n",
    "            All_metrics[model_name]['prediction']['values'].append(syntax_checker.renderer.render(prediction))\n",
    "            All_metrics[model_name]['f1']['values'].append(f1)\n",
    "            All_metrics[model_name]['time']['values'].append(duration)\n",
    "            \n",
    "        for metric in All_metrics[model_name]:\n",
    "            if metric != 'prediction':\n",
    "                All_metrics[model_name][metric]['mean'] = [np.mean(All_metrics[model_name][metric]['values'])]\n",
    "                All_metrics[model_name][metric]['std'] = [np.std(All_metrics[model_name][metric]['values'])]\n",
    "        \n",
    "        print(model_name+' Speed: {}s +- {} / lp'.format(round(All_metrics[model_name]['time']['mean'][0], 2),\\\n",
    "                                                               round(All_metrics[model_name]['time']['std'][0], 2)))\n",
    "        print(model_name+' Avg Acc: {}% +- {} / lp'.format(round(All_metrics[model_name]['acc']['mean'][0], 2),\\\n",
    "                                                               round(All_metrics[model_name]['acc']['std'][0], 2)))\n",
    "        print(model_name+' Avg F1: {}% +- {} / lp'.format(round(All_metrics[model_name]['f1']['mean'][0], 2),\\\n",
    "                                                               round(All_metrics[model_name]['f1']['std'][0], 2)))\n",
    "#        print(model_name+' Avg Str_Acc: {}% +- {} / lp'.format(round(All_metrics[model_name]['str_acc']['mean'][0], 2),\\\n",
    "#                                                               round(All_metrics[model_name]['str_acc']['std'][0], 2)))\n",
    "#        print(\"\\n\")\n",
    "        print()\n",
    "        \n",
    "        with open(\"datasets/\"+kb_name+\"/Results/NCES\"+desc+\".json\", \"w\") as file:\n",
    "            json.dump(All_metrics, file, indent=3, ensure_ascii=False)\n",
    "    return All_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fifteen-briefing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#F1_semb = evaluate_nces(kb_path_semb, kwargs, kb_name='semantic_bible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "editorial-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1_semb['SetTransformer']['f1']['values']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-greensboro",
   "metadata": {},
   "source": [
    "## Carcinogenesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "urban-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on carcinogenesis KB:\n",
      "##################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB namespace:  http://dl-learner.org/carcinogenesis#\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 85.77870542156258%, Hard: 90.06590514993873%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "SetTransformer Speed: 0.04s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 99.47% +- 2.99 / lp\n",
      "SetTransformer Avg F1: 88.45% +- 24.56 / lp\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 83.45856524427954%, Hard: 88.32843743558026%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "GRU Speed: 0.1s +- 0.0 / lp\n",
      "GRU Avg Acc: 99.79% +- 0.69 / lp\n",
      "GRU Avg F1: 89.64% +- 21.44 / lp\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 74.10482374768091%, Hard: 81.70983544933121%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "LSTM Speed: 0.11s +- 0.0 / lp\n",
      "LSTM Avg Acc: 99.73% +- 0.66 / lp\n",
      "LSTM Avg F1: 84.43% +- 23.42 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_car = evaluate_nces(\"carcinogenesis\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-crest",
   "metadata": {},
   "source": [
    "## After shuffling examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "discrete-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "progressive-animation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on carcinogenesis KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://dl-learner.org/carcinogenesis#\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 85.77870542156258%, Hard: 90.06590514993873%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "SetTransformer Speed: 0.05s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 99.47% +- 2.99 / lp\n",
      "SetTransformer Avg F1: 88.45% +- 24.56 / lp\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 82.47680890538035%, Hard: 87.53639334521685%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "GRU Speed: 0.11s +- 0.0 / lp\n",
      "GRU Avg Acc: 99.8% +- 0.71 / lp\n",
      "GRU Avg F1: 89.99% +- 20.77 / lp\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 75.09872780280945%, Hard: 81.48164893663089%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "LSTM Speed: 0.13s +- 0.0 / lp\n",
      "LSTM Avg Acc: 99.78% +- 0.61 / lp\n",
      "LSTM Avg F1: 82.14% +- 28.46 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_car_shuffle = evaluate_nces(\"carcinogenesis\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-vanilla",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "radical-harvest",
   "metadata": {},
   "source": [
    "## Model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "handy-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "phantom-customs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on carcinogenesis KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://dl-learner.org/carcinogenesis#\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 89.50371057513917%, Hard: 91.82773109243695%\n",
      "\n",
      "## Ensemble ##\n",
      "\n",
      "Ensemble Speed: 0.14s +- 0.0 / lp\n",
      "Ensemble Avg Acc: 99.87% +- 0.56 / lp\n",
      "Ensemble Avg F1: 96.08% +- 10.83 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_car_ensemble = evaluate_ensemble(\"carcinogenesis\", args, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-testament",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "realistic-junior",
   "metadata": {},
   "source": [
    "## Mutagenesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-baker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-context",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "developed-angle",
   "metadata": {},
   "source": [
    "## Family Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fifth-venue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on family-benchmark KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://www.benchmark.org/family#\n",
      "\n",
      "Number of learning problems:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 65.20184676434674%, Hard: 63.36503277200209%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "SetTransformer Speed: 0.02s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 91.4% +- 5.93 / lp\n",
      "SetTransformer Avg F1: 74.62% +- 24.47 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_fam = evaluate_nces(\"family-benchmark\", [\"SetTransformer\"], args, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-korean",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wound-scene",
   "metadata": {},
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amber-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Method.helper_functions import wilcoxon_statistical_test\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "\n",
    "def celoe_vs_nces_stat_tests():\n",
    "    with open('datasets/semantic_bible/Results/NCES.json') as file:\n",
    "        nces = json.load(file)\n",
    "    f1_nces = nces['LSTM']['f1']['values']\n",
    "    acc_nces = nces['LSTM']['acc']['values']\n",
    "    time_nces = nces['LSTM']['time']['values']\n",
    "    with open('datasets/semantic_bible/Results/concept_learning_results_celoe.json') as celoe_file:\n",
    "        celoe = json.load(celoe_file)\n",
    "    f1_celoe = celoe['F-measure']\n",
    "    acc_celoe = celoe['Accuracy']\n",
    "    time_celoe = celoe['Runtime']\n",
    "    _, p1 = wilcoxon_statistical_test(acc_nces, acc_celoe)\n",
    "    _, p2 = wilcoxon_statistical_test(f1_nces, f1_celoe)\n",
    "    _, p3 = wilcoxon_statistical_test(time_nces, time_celoe)\n",
    "    for metric, p in zip(['Accuracy', 'F1', 'RunTime'], [p1,p2,p3]):\n",
    "        print(f'On {metric} of Semantic Bible KB, p_value = ', p)\n",
    "        if p<=0.05:\n",
    "            print('Probably different distributions')\n",
    "        else:\n",
    "            print('Probably the same distribution')\n",
    "        print()\n",
    "    \n",
    "    with open('datasets/family-benchmark/Results/NCES.json') as file:\n",
    "        nces = json.load(file)\n",
    "    f1_nces = nces['LSTM']['f1']['values']\n",
    "    acc_nces = nces['LSTM']['acc']['values']\n",
    "    time_nces = nces['LSTM']['time']['values']\n",
    "    with open('datasets/family-benchmark/Results/concept_learning_results_celoe.json') as celoe_file:\n",
    "        celoe = json.load(celoe_file)\n",
    "    f1_celoe = celoe['F-measure']\n",
    "    acc_celoe = celoe['Accuracy']\n",
    "    time_celoe = celoe['Runtime']\n",
    "    _, p1 = wilcoxon_statistical_test(acc_nces, acc_celoe)\n",
    "    _, p2 = wilcoxon_statistical_test(f1_nces, f1_celoe)\n",
    "    _, p3 = wilcoxon_statistical_test(time_nces, time_celoe)\n",
    "    for metric, p in zip(['Accuracy', 'F1', 'RunTime'], [p1,p2,p3]):\n",
    "        print(f'On {metric} of Family Benchmark KB, p_value = ', p)\n",
    "        if p<=0.05:\n",
    "            print('Probably different distributions')\n",
    "        else:\n",
    "            print('Probably the same distribution')\n",
    "        print()\n",
    "        \n",
    "    with open('datasets/mutagenesis/Results/NCES.json') as file:\n",
    "        nces = json.load(file)\n",
    "    f1_nces = nces['LSTM']['f1']['values']\n",
    "    acc_nces = nces['LSTM']['acc']['values']\n",
    "    time_nces = nces['LSTM']['time']['values']\n",
    "    with open('datasets/mutagenesis/Results/concept_learning_results_celoe.json') as celoe_file:\n",
    "        celoe = json.load(celoe_file)\n",
    "    f1_celoe = celoe['F-measure']\n",
    "    acc_celoe = celoe['Accuracy']\n",
    "    time_celoe = celoe['Runtime']\n",
    "    _, p1 = wilcoxon_statistical_test(acc_nces, acc_celoe)\n",
    "    _, p2 = wilcoxon_statistical_test(f1_nces, f1_celoe)\n",
    "    _, p3 = wilcoxon_statistical_test(time_nces, time_celoe)\n",
    "    for metric, p in zip(['Accuracy', 'F1', 'RunTime'], [p1,p2,p3]):\n",
    "        print(f'On {metric} of Mutagenesis KB, p_value = ', p)\n",
    "        if p<=0.05:\n",
    "            print('Probably different distributions')\n",
    "        else:\n",
    "            print('Probably the same distribution')\n",
    "        print()\n",
    "        \n",
    "    with open('datasets/carcinogenesis/Results/NCES.json') as file:\n",
    "        nces = json.load(file)\n",
    "    f1_nces = nces['LSTM']['f1']['values']\n",
    "    acc_nces = nces['LSTM']['acc']['values']\n",
    "    time_nces = nces['LSTM']['time']['values']\n",
    "    with open('datasets/carcinogenesis/Results/concept_learning_results_celoe.json') as celoe_file:\n",
    "        celoe = json.load(celoe_file)\n",
    "    f1_celoe = celoe['F-measure']\n",
    "    acc_celoe = celoe['Accuracy']\n",
    "    time_celoe = celoe['Runtime']\n",
    "    _, p1 = wilcoxon_statistical_test(acc_nces, acc_celoe)\n",
    "    _, p2 = wilcoxon_statistical_test(f1_nces, f1_celoe)\n",
    "    _, p3 = wilcoxon_statistical_test(time_nces, time_celoe)\n",
    "    for metric, p in zip(['Accuracy', 'F1', 'RunTime'], [p1,p2,p3]):\n",
    "        print(f'On {metric} of Carcinogenesis KB, p_value = ', p)\n",
    "        if p<=0.05:\n",
    "            print('Probably different distributions')\n",
    "        else:\n",
    "            print('Probably the same distribution')\n",
    "        print()\n",
    "        \n",
    "    with open('datasets/vicodi/Results/NCES.json') as file:\n",
    "        nces = json.load(file)\n",
    "    f1_nces = nces['LSTM']['f1']['values']\n",
    "    acc_nces = nces['LSTM']['acc']['values']\n",
    "    time_nces = nces['LSTM']['time']['values']\n",
    "    with open('datasets/vicodi/Results/concept_learning_results_celoe.json') as celoe_file:\n",
    "        celoe = json.load(celoe_file)\n",
    "    f1_celoe = celoe['F-measure']\n",
    "    acc_celoe = celoe['Accuracy']\n",
    "    time_celoe = celoe['Runtime']\n",
    "    _, p1 = wilcoxon_statistical_test(acc_nces, acc_celoe)\n",
    "    _, p2 = wilcoxon_statistical_test(f1_nces, f1_celoe)\n",
    "    _, p3 = wilcoxon_statistical_test(time_nces, time_celoe)\n",
    "    for metric, p in zip(['Accuracy', 'F1', 'RunTime'], [p1,p2,p3]):\n",
    "        print(f'On {metric} of Vicodi KB, p_value = ', p)\n",
    "        if p<=0.05:\n",
    "            print('Probably different distributions')\n",
    "        else:\n",
    "            print('Probably the same distribution')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "simple-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "#celoe_vs_nces_stat_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-vacation",
   "metadata": {},
   "source": [
    "## Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "plt_data_path = \"datasets/semantic_bible/Plot_data/plot_data.json\"\n",
    "with open(plt_data_path,\"r\") as plt_file1:\n",
    "    plt_data1 = json.load(plt_file1)\n",
    "\n",
    "plt_data_path = \"datasets/family-benchmark/Plot_data/plot_data.json\"\n",
    "with open(plt_data_path,\"r\") as plt_file2:\n",
    "    plt_data2 = json.load(plt_file2)\n",
    "    \n",
    "    \n",
    "plt_data_path = \"datasets/carcinogenesis/Plot_data/plot_data.json\"\n",
    "with open(plt_data_path,\"r\") as plt_file3:\n",
    "    plt_data3 = json.load(plt_file3)\n",
    "\n",
    "plt_data_path = \"datasets/mutagenesis/Plot_data/plot_data.json\"\n",
    "with open(plt_data_path,\"r\") as plt_file4:\n",
    "    plt_data4 = json.load(plt_file4)\n",
    "    \n",
    "plt_data_path = \"datasets/vicodi/Plot_data/plot_data.json\"\n",
    "with open(plt_data_path,\"r\") as plt_file5:\n",
    "    plt_data5 = json.load(plt_file5)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(plt_data1, plt_data2, name1, name2):\n",
    "\n",
    "    Markers = ['--', ':', '-']\n",
    "    Colors = ['g', 'b', 'm']\n",
    "    i = 0\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10,7))\n",
    "    #fig.suptitle('Sharing x per column, y per row')\n",
    "\n",
    "    for crv in plt_data1['loss']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax1.plot(crv, mk, markersize=5, color=c)\n",
    "        i += 1\n",
    "    ax1.legend(('GRU', 'LSTM', 'CNN'))\n",
    "    ax1.set_title(name1)\n",
    "    ax1.set(ylabel='Loss')\n",
    "\n",
    "    for crv in plt_data2['loss']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax2.plot(crv, mk, markersize=5, color=c)\n",
    "        i += 1   \n",
    "    ax2.legend(('GRU', 'LSTM', 'CNN'))\n",
    "    ax2.set_title(name2)\n",
    "\n",
    "    for crv in plt_data1['hard acc']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax3.plot(crv, mk, markersize=5, color=c)\n",
    "        i += 1\n",
    "    ax3.legend(('GRU', 'LSTM', 'CNN'))\n",
    "    ax3.set(ylabel='Hard Accuracy', xlabel='Epochs')    \n",
    "\n",
    "    for crv in plt_data2['hard acc']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax4.plot(crv, mk, markersize=5, color=c)\n",
    "        i += 1\n",
    "    ax4.legend(('GRU', 'LSTM', 'CNN'))\n",
    "    ax4.set(xlabel='Epochs')    \n",
    "\n",
    "    for ax in fig.get_axes():\n",
    "        ax.label_outer()\n",
    "    fig.savefig('training-curves'+name1+'_and_'+name2+'.pdf')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(plt_data1, plt_data2, name1='Semantic Bible', name2='Family Benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(plt_data3, plt_data4, name1='Carcinogenesis', name2='Mutagenesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-commons",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_acc_curves(plt_data1, plt_data2, plt_data3, plt_data4, plt_data5, name1, name2, name3, name4, name5, mode='hard'):\n",
    "\n",
    "    Markers = ['--', ':', '-']\n",
    "    Colors = ['g', 'b', 'm']\n",
    "    i = 0\n",
    "    fig, ((ax1, ax2, ax3, ax4, ax5)) = plt.subplots(1, 5, figsize=(30,6), sharey=True)\n",
    "    #fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(15,5), sharey=True, sharex=True)\n",
    "    #fig.suptitle('Sharing x per column, y per row')\n",
    "\n",
    "    for crv in plt_data1[f'{mode} acc']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax1.plot(crv, mk, markersize=6, color=c)\n",
    "        i += 1\n",
    "    leg1 = ax1.legend(('GRU', 'LSTM', 'CNN'), prop={'size': 20})\n",
    "    for line in leg1.get_lines():\n",
    "        line.set_linewidth(4.0)\n",
    "    ax1.set_title(name1, fontsize=30, fontweight=\"bold\")\n",
    "    ax1.set_xlabel('Epochs', fontsize=25)\n",
    "    ax1.set_ylabel(mode.capitalize()+' Accuracy', fontsize=25)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    for crv in plt_data2[f'{mode} acc']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax2.plot(crv, mk, markersize=6, color=c)\n",
    "        i += 1   \n",
    "    leg2 = ax2.legend(('GRU', 'LSTM', 'CNN'), prop={'size': 20})\n",
    "    for line in leg2.get_lines():\n",
    "        line.set_linewidth(4.0)\n",
    "    ax2.set_title(name2, fontsize=30, fontweight=\"bold\")\n",
    "    ax2.set_xlabel('Epochs', fontsize=25)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    for crv in plt_data3[f'{mode} acc']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax3.plot(crv, mk, markersize=10, color=c)\n",
    "        i += 1\n",
    "    leg3 = ax3.legend(('GRU', 'LSTM', 'CNN'), prop={'size': 20})\n",
    "    for line in leg3.get_lines():\n",
    "        line.set_linewidth(4.0)\n",
    "    ax3.set_title(name3, fontsize=30, fontweight=\"bold\")\n",
    "    ax3.set_xlabel('Epochs', fontsize=25)\n",
    "    ax3.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    for crv in plt_data4[f'{mode} acc']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax4.plot(crv, mk, markersize=10, color=c)\n",
    "        i += 1\n",
    "    leg4 = ax4.legend(('GRU', 'LSTM', 'CNN'), prop={'size': 20})\n",
    "    for line in leg4.get_lines():\n",
    "        line.set_linewidth(4.0)\n",
    "    ax4.set_xlabel('Epochs', fontsize=25)\n",
    "    ax4.set_title(name4, fontsize=30, fontweight=\"bold\")\n",
    "    ax4.tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    for crv in plt_data5[f'{mode} acc']:\n",
    "        mk = Markers[i%3]\n",
    "        c = Colors[i%3]\n",
    "        ax5.plot(crv, mk, markersize=10, color=c)\n",
    "        i += 1\n",
    "    \n",
    "    leg5 = ax5.legend(('GRU', 'LSTM', 'CNN'), prop={'size': 20})\n",
    "    for line in leg5.get_lines():\n",
    "        line.set_linewidth(4.0)\n",
    "    ax5.set_xlabel('Epochs', fontsize=25)\n",
    "    ax5.set_title(name5, fontsize=30, fontweight=\"bold\")\n",
    "    ax5.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    for ax in fig.get_axes():\n",
    "        ax.label_outer()\n",
    "    fig.savefig(f'accuracy-curves-all-KBs_{mode}.pdf', bbox_inches='tight')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-priest",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name1, name2, name3, name4, name5 = 'Semantic Bible', 'Family Benchmark', 'Carcinogenesis', 'Mutagenesis', 'Vicodi'\n",
    "plot_acc_curves(plt_data1, plt_data2, plt_data3, plt_data4, plt_data5, name1, name2, name3, name4, name5, mode='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-habitat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nces",
   "language": "python",
   "name": "nces"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
