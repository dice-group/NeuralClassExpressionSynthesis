{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "inside-bundle",
   "metadata": {},
   "source": [
    "## Evaluate NCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "careful-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from utils.syntax_checker import SyntaxChecker\n",
    "from utils.evaluator import Evaluator\n",
    "from ontolearn.knowledge_base import KnowledgeBase\n",
    "from nces import BaseConceptSynthesis\n",
    "from nces.synthesizer import ConceptSynthesizer\n",
    "from utils.data import Data\n",
    "from owlapy.parser import DLSyntaxParser\n",
    "from dataloader import CSDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "limited-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import json\n",
    "import torch, pandas as pd\n",
    "with open(\"settings.json\") as setting:\n",
    "    args = json.load(setting)\n",
    "args = Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compact-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interstate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_pad(arg):\n",
    "    arg_temp = []\n",
    "    for atm in arg:\n",
    "        if atm == 'PAD':\n",
    "            break\n",
    "        arg_temp.append(atm)\n",
    "    return arg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adverse-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(prediction, target):\n",
    "    def soft(arg1, arg2):\n",
    "        arg1_ = arg1\n",
    "        arg2_ = arg2\n",
    "        if isinstance(arg1_, str):\n",
    "            arg1_ = set(before_pad(BaseConceptSynthesis.decompose(arg1_)))\n",
    "        else:\n",
    "            arg1_ = set(before_pad(arg1_))\n",
    "        if isinstance(arg2_, str):\n",
    "            arg2_ = set(before_pad(BaseConceptSynthesis.decompose(arg2_)))\n",
    "        else:\n",
    "            arg2_ = set(before_pad(arg2_))\n",
    "        return 100*float(len(arg1_.intersection(arg2_)))/len(arg1_.union(arg2_))\n",
    "\n",
    "    def hard(arg1, arg2):\n",
    "        arg1_ = arg1\n",
    "        arg2_ = arg2\n",
    "        if isinstance(arg1_, str):\n",
    "            arg1_ = before_pad(BaseConceptSynthesis.decompose(arg1_))\n",
    "        else:\n",
    "            arg1_ = before_pad(arg1_)\n",
    "        if isinstance(arg2_, str):\n",
    "            arg2_ = before_pad(BaseConceptSynthesis.decompose(arg2_))\n",
    "        else:\n",
    "            arg2_ = before_pad(arg2_)\n",
    "        return 100*float(sum(map(lambda x,y: x==y, arg1_, arg2_)))/max(len(arg1_), len(arg2_))\n",
    "    soft_acc = sum(map(soft, prediction, target))/len(target)\n",
    "    hard_acc = sum(map(hard, prediction, target))/len(target)\n",
    "    return soft_acc, hard_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "formal-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_examples(pos, neg, num_examples):\n",
    "    if min(len(neg),len(pos)) >= num_examples//2:\n",
    "        if len(pos) > len(neg):\n",
    "            num_neg_ex = num_examples//2\n",
    "            num_pos_ex = num_examples-num_neg_ex\n",
    "        else:\n",
    "            num_pos_ex = num_examples//2\n",
    "            num_neg_ex = num_examples-num_pos_ex\n",
    "    elif len(pos) > len(neg):\n",
    "        num_neg_ex = len(neg)\n",
    "        num_pos_ex = num_examples-num_neg_ex\n",
    "    elif len(pos) < len(neg):\n",
    "        num_pos_ex = len(pos)\n",
    "        num_neg_ex = num_examples-num_pos_ex\n",
    "    positive = random.sample(pos, num_pos_ex)\n",
    "    negative = random.sample(neg, num_neg_ex)\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "virgin-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_token(model, idx_array):\n",
    "    return model.inv_vocab[idx_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "located-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    pos_emb_list = []\n",
    "    neg_emb_list = []\n",
    "    target_tokens_list = []\n",
    "    target_labels = []\n",
    "    for pos_emb, neg_emb, label in batch:\n",
    "        pos_emb_list.append(pos_emb)\n",
    "        neg_emb_list.append(neg_emb)\n",
    "        target_labels.append(label)\n",
    "    pos_emb_list = pad_sequence(pos_emb_list, batch_first=True, padding_value=0)\n",
    "    neg_emb_list = pad_sequence(neg_emb_list, batch_first=True, padding_value=0)\n",
    "    target_labels = pad_sequence(target_labels, batch_first=True, padding_value=-100)\n",
    "    return pos_emb_list, neg_emb_list, target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "documented-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(kb, embeddings, kwargs):\n",
    "    data_test_path = f\"datasets/{kb}/Test_data/Data.json\"\n",
    "    with open(data_test_path, \"r\") as file:\n",
    "        data_test = json.load(file)\n",
    "    data_test = list(data_test.items())\n",
    "    test_dataset = CSDataLoader(data_test, embeddings, kwargs)\n",
    "    print(\"Number of learning problems: \", len(test_dataset))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=kwargs.batch_size, num_workers=kwargs.num_workers, collate_fn=collate_batch, shuffle=False)\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "basic-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_prediction(models, x1, x2):\n",
    "    for i,model in enumerate(models):\n",
    "        model.eval()\n",
    "        if i == 0:\n",
    "            _, scores = model(x1, x2)\n",
    "        else:\n",
    "            _, sc = model(x1, x2)\n",
    "            scores = scores + sc\n",
    "    scores = scores/len(models)\n",
    "    prediction = model.inv_vocab[scores.argmax(1)]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "existing-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_expressions(model_name, kb, args, ensemble=False):\n",
    "    args.knowledge_base_path = \"datasets/\"+f\"{kb}/{kb}.owl\"\n",
    "    embeddings = pd.read_csv(f\"embeddings/{kb}/ConEx_entity_embeddings.csv\").set_index('Unnamed: 0')\n",
    "    dataloader = get_data(kb, embeddings, args)\n",
    "    if ensemble:\n",
    "        models = [torch.load(f\"datasets/{kb}/Model_weights/{name}.pt\", map_location=torch.device('cpu'))\\\n",
    "                  for name in [\"SetTransformer\", \"GRU\", \"LSTM\"]]\n",
    "    if not ensemble:\n",
    "        model = torch.load(f\"datasets/{kb}/Model_weights/{model_name}.pt\", map_location=torch.device('cpu'))\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = models[0]\n",
    "        model.eval()\n",
    "    soft_acc, hard_acc = 0.0, 0.0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for x1, x2, labels in tqdm(dataloader):\n",
    "        target_sequence = map_to_token(model, labels)\n",
    "        if ensemble:\n",
    "            pred_sequence = get_ensemble_prediction(models, x1, x2)\n",
    "        else:\n",
    "            pred_sequence, _ = model(x1, x2)\n",
    "        preds.append(pred_sequence)\n",
    "        targets.append(target_sequence)\n",
    "        s_acc, h_acc = compute_accuracy(pred_sequence, target_sequence)\n",
    "        soft_acc += s_acc\n",
    "        hard_acc += h_acc\n",
    "    print(f\"Average syntactic accuracy, Soft: {soft_acc/len(dataloader)}%, Hard: {hard_acc/len(dataloader)}%\")\n",
    "    return np.concatenate(preds, 0), np.concatenate(targets, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unknown-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(kb_name, args, verbose=False):\n",
    "    print('#'*50)\n",
    "    print('NCES evaluation on {} KB:'.format(kb_name))\n",
    "    print('#'*50)\n",
    "    All_metrics = {\"Ensemble\": defaultdict(lambda: defaultdict(list))}\n",
    "    print()\n",
    "    kb = KnowledgeBase(path=f\"datasets/{kb_name}/{kb_name}.owl\")\n",
    "    namespace = kb.ontology()._onto.base_iri\n",
    "    if kb_name == 'family-benchmark':\n",
    "        namespace = 'http://www.benchmark.org/family#'\n",
    "    if kb_name == 'vicodi':\n",
    "        namespace = 'http://vicodi.org/ontology#'\n",
    "    print(\"KB namespace: \", namespace)\n",
    "    print()\n",
    "    syntax_checker = SyntaxChecker(kb)\n",
    "    evaluator = Evaluator(kb)\n",
    "    dl_parser = DLSyntaxParser(namespace = namespace)\n",
    "    All_individuals = set(kb.individuals())\n",
    "    with open(f\"datasets/{kb_name}/Test_data/Data.json\", \"r\") as file:\n",
    "        data_test = json.load(file)\n",
    "        \n",
    "    t0 = time.time()\n",
    "    predictions, targets = predict_class_expressions(\"Ensemble\", kb_name, args, ensemble=True)\n",
    "    t1 = time.time()\n",
    "    duration = (t1-t0)/len(predictions)\n",
    "    print()\n",
    "    print(f\"## Ensemble ##\")\n",
    "    print()\n",
    "    for i, pb_str in enumerate(targets):\n",
    "        pb_str = \"\".join(before_pad(pb_str))\n",
    "        #examples = data_test[pb_str]\n",
    "        #pos_examples = set(examples['positive examples'])\n",
    "        #neg_examples = set(examples['negative examples'])\n",
    "        try:\n",
    "            end_idx = np.where(predictions[i] == 'PAD')[0][0] # remove padding token\n",
    "        except IndexError:\n",
    "            end_idx = 1\n",
    "        pred = predictions[i][:end_idx]\n",
    "        #print(\"Before parsing: \", pred.sum())\n",
    "        succeed = False\n",
    "        if (pred=='(').sum() > (pred==')').sum():\n",
    "            for i in range(len(pred))[::-1]:\n",
    "                try:\n",
    "                    prediction = dl_parser.parse_expression(\"\".join(pred.tolist().insert(i,')')))\n",
    "                    succeed = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not succeed:\n",
    "                try:\n",
    "                    pred = syntax_checker.correct(pred.sum())\n",
    "                    pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                    prediction = syntax_checker.get_concept(pred)\n",
    "                except Exception:\n",
    "                    print(f\"Could not understand expression {pred}\")\n",
    "                    continue\n",
    "        elif (pred==')').sum() > (pred=='(').sum():\n",
    "            for i in range(len(pred)):\n",
    "                try:\n",
    "                    prediction = dl_parser.parse_expression(\"\".join(pred.tolist().insert(i,'(')))\n",
    "                    succeed = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not succeed:\n",
    "                try:\n",
    "                    pred = syntax_checker.correct(pred.sum())\n",
    "                    pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                    prediction = syntax_checker.get_concept(pred)\n",
    "                except Exception:\n",
    "                    print(f\"Could not understand expression {pred}\")\n",
    "                    continue\n",
    "        else:\n",
    "            try:\n",
    "                prediction = dl_parser.parse_expression(\"\".join(pred.tolist()))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    pred = syntax_checker.correct(pred.sum())\n",
    "                    pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                    prediction = syntax_checker.get_concept(pred)\n",
    "                except Exception:\n",
    "                    print(f\"Could not understand expression {pred}\")\n",
    "                    continue\n",
    "        target_expression = dl_parser.parse_expression(pb_str) # The target class expression\n",
    "        try:\n",
    "            positive_examples = {ind.get_iri().as_str().split(\"/\")[-1] for ind in kb.individuals(target_expression)}\n",
    "            negative_examples = All_individuals-positive_examples\n",
    "            acc, f1 = evaluator.evaluate(prediction, positive_examples, negative_examples)\n",
    "        except NotImplementedError:\n",
    "            print(\"Invalid target or predicted expression, skipping\")\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f'Problem {i}, Target: {pb_str}, Prediction: {syntax_checker.renderer.render(prediction)}, Acc: {acc}, F1: {f1}')\n",
    "            print()\n",
    "        All_metrics[\"Ensemble\"]['acc']['values'].append(acc)\n",
    "        All_metrics[\"Ensemble\"]['prediction']['values'].append(syntax_checker.renderer.render(prediction))\n",
    "        All_metrics[\"Ensemble\"]['f1']['values'].append(f1)\n",
    "        All_metrics[\"Ensemble\"]['time']['values'].append(duration)\n",
    "\n",
    "    for metric in All_metrics[\"Ensemble\"]:\n",
    "        if metric != 'prediction':\n",
    "            All_metrics[\"Ensemble\"][metric]['mean'] = [np.mean(All_metrics[\"Ensemble\"][metric]['values'])]\n",
    "            All_metrics[\"Ensemble\"][metric]['std'] = [np.std(All_metrics[\"Ensemble\"][metric]['values'])]\n",
    "\n",
    "    print(\"Ensemble\"+' Speed: {}s +- {} / lp'.format(round(All_metrics[\"Ensemble\"]['time']['mean'][0], 2),\\\n",
    "                                                           round(All_metrics[\"Ensemble\"]['time']['std'][0], 2)))\n",
    "    print(\"Ensemble\"+' Avg Acc: {}% +- {} / lp'.format(round(All_metrics[\"Ensemble\"]['acc']['mean'][0], 2),\\\n",
    "                                                           round(All_metrics[\"Ensemble\"]['acc']['std'][0], 2)))\n",
    "    print(\"Ensemble\"+' Avg F1: {}% +- {} / lp'.format(round(All_metrics[\"Ensemble\"]['f1']['mean'][0], 2),\\\n",
    "                                                           round(All_metrics[\"Ensemble\"]['f1']['std'][0], 2)))\n",
    "#        print(\"Ensemble\"+' Avg Str_Acc: {}% +- {} / lp'.format(round(All_metrics[\"Ensemble\"]['str_acc']['mean'][0], 2),\\\n",
    "#                                                               round(All_metrics[\"Ensemble\"]['str_acc']['std'][0], 2)))\n",
    "    \n",
    "    print()\n",
    "\n",
    "    with open(\"datasets/\"+kb_name+\"/Results/NCES_Ensemble.json\", \"w\") as file:\n",
    "        json.dump(All_metrics, file, indent=3, ensure_ascii=False)\n",
    "    return All_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "paperback-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nces(kb_name, models, args, verbose=False):\n",
    "    print('#'*50)\n",
    "    print('NCES evaluation on {} KB:'.format(kb_name))\n",
    "    print('#'*50)\n",
    "    desc = \"\"\n",
    "    if args.shuffle_examples:\n",
    "        desc = \"_shuffle\"\n",
    "    All_metrics = {m: defaultdict(lambda: defaultdict(list)) for m in models}\n",
    "    print()\n",
    "    kb = KnowledgeBase(path=f\"datasets/{kb_name}/{kb_name}.owl\")\n",
    "    namespace = kb.ontology()._onto.base_iri\n",
    "    if kb_name == 'family-benchmark':\n",
    "        namespace = 'http://www.benchmark.org/family#'\n",
    "    if kb_name == 'vicodi':\n",
    "        namespace = 'http://vicodi.org/ontology#'\n",
    "    print(\"KB namespace: \", namespace)\n",
    "    print()\n",
    "    syntax_checker = SyntaxChecker(kb)\n",
    "    evaluator = Evaluator(kb)\n",
    "    dl_parser = DLSyntaxParser(namespace = namespace)\n",
    "    All_individuals = set(kb.individuals())\n",
    "    with open(f\"datasets/{kb_name}/Test_data/Data.json\", \"r\") as file:\n",
    "        data_test = json.load(file)\n",
    "    for model_name in models:\n",
    "        t0 = time.time()\n",
    "        predictions, targets = predict_class_expressions(model_name, kb_name, args)\n",
    "        t1 = time.time()\n",
    "        duration = (t1-t0)/len(predictions)\n",
    "        print()\n",
    "        print(f\"##{model_name}##\")\n",
    "        print()\n",
    "        for i, pb_str in enumerate(targets):\n",
    "            pb_str = \"\".join(before_pad(pb_str))\n",
    "            #examples = data_test[pb_str]\n",
    "            #pos_examples = set(examples['positive examples'])\n",
    "            #neg_examples = set(examples['negative examples'])\n",
    "            try:\n",
    "                end_idx = np.where(predictions[i] == 'PAD')[0][0] # remove padding token\n",
    "            except IndexError:\n",
    "                end_idx = 1\n",
    "            pred = predictions[i][:end_idx]\n",
    "            #print(\"Before parsing: \", pred.sum())\n",
    "            succeed = False\n",
    "            if (pred=='(').sum() > (pred==')').sum():\n",
    "                for i in range(len(pred))[::-1]:\n",
    "                    try:\n",
    "                        prediction = dl_parser.parse_expression(\"\".join(pred.tolist().insert(i,')')))\n",
    "                        succeed = True\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if not succeed:\n",
    "                    try:\n",
    "                        pred = syntax_checker.correct(pred.sum())\n",
    "                        pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                        prediction = syntax_checker.get_concept(pred)\n",
    "                    except Exception:\n",
    "                        print(f\"Could not understand expression {pred}\")\n",
    "                        continue\n",
    "            elif (pred==')').sum() > (pred=='(').sum():\n",
    "                for i in range(len(pred)):\n",
    "                    try:\n",
    "                        prediction = dl_parser.parse_expression(\"\".join(pred.tolist().insert(i,'(')))\n",
    "                        succeed = True\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if not succeed:\n",
    "                    try:\n",
    "                        pred = syntax_checker.correct(pred.sum())\n",
    "                        pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                        prediction = syntax_checker.get_concept(pred)\n",
    "                    except Exception:\n",
    "                        print(f\"Could not understand expression {pred}\")\n",
    "                        continue\n",
    "            else:\n",
    "                try:\n",
    "                    prediction = dl_parser.parse_expression(\"\".join(pred.tolist()))\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        pred = syntax_checker.correct(pred.sum())\n",
    "                        pred = list(syntax_checker.get_suggestions(pred))[-1]\n",
    "                        prediction = syntax_checker.get_concept(pred)\n",
    "                    except Exception:\n",
    "                        print(f\"Could not understand expression {pred}\")\n",
    "                        continue\n",
    "            target_expression = dl_parser.parse_expression(pb_str) # The target class expression\n",
    "            try:\n",
    "                positive_examples = {ind.get_iri().as_str().split(\"/\")[-1] for ind in kb.individuals(target_expression)}\n",
    "                negative_examples = All_individuals-positive_examples\n",
    "                acc, f1 = evaluator.evaluate(prediction, positive_examples, negative_examples)\n",
    "                syntax_checker.renderer.render(prediction)\n",
    "            except Exception:\n",
    "                print(\"Invalid target or predicted expression, skipping\")\n",
    "                continue\n",
    "            if verbose:\n",
    "                print(f'Problem {i}, Target: {pb_str}, Prediction: {syntax_checker.renderer.render(prediction)}, Acc: {acc}, F1: {f1}')\n",
    "                print()\n",
    "            All_metrics[model_name]['acc']['values'].append(acc)\n",
    "            All_metrics[model_name]['prediction']['values'].append(syntax_checker.renderer.render(prediction))\n",
    "            All_metrics[model_name]['f1']['values'].append(f1)\n",
    "            All_metrics[model_name]['time']['values'].append(duration)\n",
    "            \n",
    "        for metric in All_metrics[model_name]:\n",
    "            if metric != 'prediction':\n",
    "                All_metrics[model_name][metric]['mean'] = [np.mean(All_metrics[model_name][metric]['values'])]\n",
    "                All_metrics[model_name][metric]['std'] = [np.std(All_metrics[model_name][metric]['values'])]\n",
    "        \n",
    "        print(model_name+' Speed: {}s +- {} / lp'.format(round(All_metrics[model_name]['time']['mean'][0], 2),\\\n",
    "                                                               round(All_metrics[model_name]['time']['std'][0], 2)))\n",
    "        print(model_name+' Avg Acc: {}% +- {} / lp'.format(round(All_metrics[model_name]['acc']['mean'][0], 2),\\\n",
    "                                                               round(All_metrics[model_name]['acc']['std'][0], 2)))\n",
    "        print(model_name+' Avg F1: {}% +- {} / lp'.format(round(All_metrics[model_name]['f1']['mean'][0], 2),\\\n",
    "                                                               round(All_metrics[model_name]['f1']['std'][0], 2)))\n",
    "#        print(model_name+' Avg Str_Acc: {}% +- {} / lp'.format(round(All_metrics[model_name]['str_acc']['mean'][0], 2),\\\n",
    "#                                                               round(All_metrics[model_name]['str_acc']['std'][0], 2)))\n",
    "#        print(\"\\n\")\n",
    "        print()\n",
    "        \n",
    "        with open(\"datasets/\"+kb_name+\"/Results/NCES\"+desc+\".json\", \"w\") as file:\n",
    "            json.dump(All_metrics, file, indent=3, ensure_ascii=False)\n",
    "    return All_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-salmon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "individual-package",
   "metadata": {},
   "source": [
    "# Carcinogenesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "recognized-obligation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on carcinogenesis KB:\n",
      "##################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB namespace:  http://dl-learner.org/carcinogenesis#\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 83.99277026828048%, Hard: 88.87130609819683%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "SetTransformer Speed: 0.04s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 99.54% +- 2.94 / lp\n",
      "SetTransformer Avg F1: 87.41% +- 25.34 / lp\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 80.9322820037106%, Hard: 84.612996713837%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "GRU Speed: 0.09s +- 0.0 / lp\n",
      "GRU Avg Acc: 99.81% +- 0.63 / lp\n",
      "GRU Avg F1: 87.02% +- 25.11 / lp\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 74.26252319109464%, Hard: 81.5938496610765%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "LSTM Speed: 0.11s +- 0.0 / lp\n",
      "LSTM Avg Acc: 99.69% +- 0.78 / lp\n",
      "LSTM Avg F1: 82.46% +- 25.41 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_car = evaluate_nces(\"carcinogenesis\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-vocabulary",
   "metadata": {},
   "source": [
    "### After shuffling examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "consistent-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "colored-container",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on carcinogenesis KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://dl-learner.org/carcinogenesis#\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 83.99277026828048%, Hard: 88.87130609819683%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "SetTransformer Speed: 0.05s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 99.54% +- 2.94 / lp\n",
      "SetTransformer Avg F1: 87.41% +- 25.34 / lp\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 82.22170686456401%, Hard: 86.21763857057972%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "GRU Speed: 0.12s +- 0.0 / lp\n",
      "GRU Avg Acc: 99.77% +- 0.65 / lp\n",
      "GRU Avg F1: 86.22% +- 26.58 / lp\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 73.32405689548548%, Hard: 81.96183018662006%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "LSTM Speed: 0.12s +- 0.0 / lp\n",
      "LSTM Avg Acc: 99.6% +- 1.05 / lp\n",
      "LSTM Avg F1: 79.76% +- 29.83 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_car_shuffle = evaluate_nces(\"carcinogenesis\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-courage",
   "metadata": {},
   "source": [
    "### Model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "absolute-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "assisted-measurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on carcinogenesis KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://dl-learner.org/carcinogenesis#\n",
      "\n",
      "Number of learning problems:  98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 89.04298082869512%, Hard: 91.59430438842202%\n",
      "\n",
      "## Ensemble ##\n",
      "\n",
      "Ensemble Speed: 0.13s +- 0.0 / lp\n",
      "Ensemble Avg Acc: 99.93% +- 0.31 / lp\n",
      "Ensemble Avg F1: 96.13% +- 10.65 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_car_ensemble = evaluate_ensemble(\"carcinogenesis\", args, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-neighbor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dependent-station",
   "metadata": {},
   "source": [
    "# Mutagenesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accessible-chicken",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on mutagenesis KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://dl-learner.org/mutagenesis#\n",
      "\n",
      "Number of learning problems:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n",
      "Object Complement Of not implemented at OWLObjectComplementOf(OWLClass(IRI('http://dl-learner.org/mutagenesis#','Bond-2')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 77.1659779614325%, Hard: 81.58861340679523%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "SetTransformer Speed: 0.17s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 99.9% +- 0.19 / lp\n",
      "SetTransformer Avg F1: 87.88% +- 26.34 / lp\n",
      "\n",
      "Number of learning problems:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 64.56709956709956%, Hard: 71.95018365472912%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "GRU Speed: 0.23s +- 0.0 / lp\n",
      "GRU Avg Acc: 99.12% +- 2.66 / lp\n",
      "GRU Avg F1: 60.64% +- 45.42 / lp\n",
      "\n",
      "Number of learning problems:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 60.954348681621404%, Hard: 68.00177095631642%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "LSTM Speed: 0.24s +- 0.0 / lp\n",
      "LSTM Avg Acc: 99.94% +- 0.1 / lp\n",
      "LSTM Avg F1: 78.64% +- 39.35 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_mut = evaluate_nces(\"mutagenesis\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-failure",
   "metadata": {},
   "source": [
    "### After shuffling examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "applicable-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lovely-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on mutagenesis KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://dl-learner.org/mutagenesis#\n",
      "\n",
      "Number of learning problems:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "Object Complement Of not implemented at OWLObjectComplementOf(OWLClass(IRI('http://dl-learner.org/mutagenesis#','Bond-2')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 77.1659779614325%, Hard: 81.58861340679523%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "SetTransformer Speed: 0.18s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 99.9% +- 0.19 / lp\n",
      "SetTransformer Avg F1: 87.88% +- 26.34 / lp\n",
      "\n",
      "Number of learning problems:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 67.94765840220386%, Hard: 68.56978879706152%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "GRU Speed: 0.27s +- 0.0 / lp\n",
      "GRU Avg Acc: 99.85% +- 0.38 / lp\n",
      "GRU Avg F1: 75.93% +- 36.45 / lp\n",
      "\n",
      "Number of learning problems:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 59.63154269972452%, Hard: 66.86179981634528%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "LSTM Speed: 0.23s +- 0.0 / lp\n",
      "LSTM Avg Acc: 99.86% +- 0.38 / lp\n",
      "LSTM Avg F1: 80.16% +- 35.68 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_mut_shuffle = evaluate_nces(\"mutagenesis\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-musical",
   "metadata": {},
   "source": [
    "### Model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dutch-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "african-glossary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on mutagenesis KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://dl-learner.org/mutagenesis#\n",
      "\n",
      "Number of learning problems:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.57s/it]\n",
      "Object Complement Of not implemented at OWLObjectComplementOf(OWLClass(IRI('http://dl-learner.org/mutagenesis#','Bond-2')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 77.12121212121211%, Hard: 81.5082644628099%\n",
      "\n",
      "## Ensemble ##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Ensemble Speed: 0.39s +- 0.0 / lp\n",
      "Ensemble Avg Acc: 99.96% +- 0.07 / lp\n",
      "Ensemble Avg F1: 89.87% +- 27.7 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_mut_ensemble = evaluate_ensemble(\"mutagenesis\", args, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-recommendation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "arranged-lucas",
   "metadata": {},
   "source": [
    "# Family Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "large-handling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on family-benchmark KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://www.benchmark.org/family#\n",
      "\n",
      "Number of learning problems:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 67.02903577903575%, Hard: 67.36635434979762%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "SetTransformer Speed: 0.03s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 89.46% +- 7.25 / lp\n",
      "SetTransformer Avg F1: 71.73% +- 20.01 / lp\n",
      "\n",
      "Number of learning problems:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 65.01602564102564%, Hard: 58.60224446549995%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "GRU Speed: 0.03s +- 0.0 / lp\n",
      "GRU Avg Acc: 90.39% +- 7.91 / lp\n",
      "GRU Avg F1: 75.68% +- 19.38 / lp\n",
      "\n",
      "Number of learning problems:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 65.34623362748363%, Hard: 63.765963126576914%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "LSTM Speed: 0.02s +- 0.0 / lp\n",
      "LSTM Avg Acc: 86.43% +- 19.56 / lp\n",
      "LSTM Avg F1: 70.99% +- 26.35 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_fam = evaluate_nces(\"family-benchmark\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-mauritius",
   "metadata": {},
   "source": [
    "### After shuffling examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "identical-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "visible-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on family-benchmark KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://www.benchmark.org/family#\n",
      "\n",
      "Number of learning problems:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 67.02903577903575%, Hard: 67.36635434979762%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "SetTransformer Speed: 0.03s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 89.46% +- 7.25 / lp\n",
      "SetTransformer Avg F1: 71.73% +- 20.01 / lp\n",
      "\n",
      "Number of learning problems:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 65.864396020646%, Hard: 59.77977887639349%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "GRU Speed: 0.02s +- 0.0 / lp\n",
      "GRU Avg Acc: 88.02% +- 15.31 / lp\n",
      "GRU Avg F1: 72.86% +- 24.45 / lp\n",
      "\n",
      "Number of learning problems:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 64.38368055555553%, Hard: 62.222663549389914%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "LSTM Speed: 0.03s +- 0.0 / lp\n",
      "LSTM Avg Acc: 89.31% +- 14.74 / lp\n",
      "LSTM Avg F1: 73.84% +- 24.74 / lp\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "F1_fam_shuffle = evaluate_nces(\"family-benchmark\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-positive",
   "metadata": {},
   "source": [
    "### Model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "vertical-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sticky-shadow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on family-benchmark KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://www.benchmark.org/family#\n",
      "\n",
      "Number of learning problems:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 70.31446273633775%, Hard: 68.9711133684919%\n",
      "\n",
      "## Ensemble ##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Ensemble Speed: 0.04s +- 0.0 / lp\n",
      "Ensemble Avg Acc: 91.89% +- 7.22 / lp\n",
      "Ensemble Avg F1: 78.06% +- 20.68 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_fam_ensemble = evaluate_ensemble(\"family-benchmark\", args, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-parameter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-nicholas",
   "metadata": {},
   "source": [
    "# Semantic Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sexual-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on semantic_bible KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://semanticbible.org/ns/2006/NTNames#\n",
      "\n",
      "Number of learning problems:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 65.36630036630036%, Hard: 77.55838932309521%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "SetTransformer Speed: 0.06s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 98.83% +- 1.09 / lp\n",
      "SetTransformer Avg F1: 78.77% +- 21.43 / lp\n",
      "\n",
      "Number of learning problems:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 66.06449106449107%, Hard: 68.71954842543079%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "GRU Speed: 0.09s +- 0.0 / lp\n",
      "GRU Avg Acc: 97.31% +- 6.7 / lp\n",
      "GRU Avg F1: 79.82% +- 28.13 / lp\n",
      "\n",
      "Number of learning problems:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 67.26807380653534%, Hard: 70.95756900104729%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "LSTM Speed: 0.08s +- 0.0 / lp\n",
      "LSTM Avg Acc: 95.29% +- 10.98 / lp\n",
      "LSTM Avg F1: 78.23% +- 30.29 / lp\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "F1_semb_shuffle = evaluate_nces(\"semantic_bible\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-leave",
   "metadata": {},
   "source": [
    "### After shuffling examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "strange-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "european-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on semantic_bible KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://semanticbible.org/ns/2006/NTNames#\n",
      "\n",
      "Number of learning problems:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 65.36630036630036%, Hard: 77.55838932309521%\n",
      "\n",
      "##SetTransformer##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "SetTransformer Speed: 0.07s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 98.83% +- 1.09 / lp\n",
      "SetTransformer Avg F1: 78.77% +- 21.43 / lp\n",
      "\n",
      "Number of learning problems:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 67.96037296037296%, Hard: 76.70186023127201%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "GRU Speed: 0.08s +- 0.0 / lp\n",
      "GRU Avg Acc: 94.98% +- 10.13 / lp\n",
      "GRU Avg F1: 75.17% +- 28.5 / lp\n",
      "\n",
      "Number of learning problems:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 58.97935397935397%, Hard: 63.218267875018455%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "LSTM Speed: 0.08s +- 0.0 / lp\n",
      "LSTM Avg Acc: 93.74% +- 12.21 / lp\n",
      "LSTM Avg F1: 68.86% +- 32.98 / lp\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "F1_semb_shuffle = evaluate_nces(\"semantic_bible\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-tyler",
   "metadata": {},
   "source": [
    "### Model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "intensive-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sublime-laundry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on semantic_bible KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://semanticbible.org/ns/2006/NTNames#\n",
      "\n",
      "Number of learning problems:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 72.22610722610723%, Hard: 77.68773709950183%\n",
      "\n",
      "## Ensemble ##\n",
      "\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Invalid target or predicted expression, skipping\n",
      "Ensemble Speed: 0.13s +- 0.0 / lp\n",
      "Ensemble Avg Acc: 97.67% +- 6.7 / lp\n",
      "Ensemble Avg F1: 84.83% +- 22.84 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_mut_ensemble = evaluate_ensemble(\"semantic_bible\", args, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-panic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-earth",
   "metadata": {},
   "source": [
    "# Vicodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "micro-andorra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on vicodi KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://vicodi.org/ontology#\n",
      "\n",
      "Number of learning problems:  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 86.07894228913334%, Hard: 89.61791008625025%\n",
      "\n",
      "##SetTransformer##\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Object Complement Of not implemented at OWLObjectComplementOf(OWLClass(IRI('http://vicodi.org/ontology#','Illness')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SetTransformer Speed: 0.05s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 99.89% +- 0.46 / lp\n",
      "SetTransformer Avg F1: 89.33% +- 23.1 / lp\n",
      "\n",
      "Number of learning problems:  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 78.61069751197141%, Hard: 85.87574887125275%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "GRU Speed: 0.13s +- 0.0 / lp\n",
      "GRU Avg Acc: 99.86% +- 0.53 / lp\n",
      "GRU Avg F1: 86.94% +- 26.93 / lp\n",
      "\n",
      "Number of learning problems:  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 73.6817658632945%, Hard: 81.62208446397271%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "LSTM Speed: 0.13s +- 0.0 / lp\n",
      "LSTM Avg Acc: 99.89% +- 0.31 / lp\n",
      "LSTM Avg F1: 84.0% +- 28.05 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_vic_shuffle = evaluate_nces(\"vicodi\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-nudist",
   "metadata": {},
   "source": [
    "### After shuffling examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "prescribed-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "attached-given",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on vicodi KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://vicodi.org/ontology#\n",
      "\n",
      "Number of learning problems:  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 86.07894228913334%, Hard: 89.61791008625025%\n",
      "\n",
      "##SetTransformer##\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Object Complement Of not implemented at OWLObjectComplementOf(OWLClass(IRI('http://vicodi.org/ontology#','Illness')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SetTransformer Speed: 0.06s +- 0.0 / lp\n",
      "SetTransformer Avg Acc: 99.89% +- 0.46 / lp\n",
      "SetTransformer Avg F1: 89.33% +- 23.1 / lp\n",
      "\n",
      "Number of learning problems:  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 81.37928419775552%, Hard: 85.17549038530674%\n",
      "\n",
      "##GRU##\n",
      "\n",
      "GRU Speed: 0.12s +- 0.0 / lp\n",
      "GRU Avg Acc: 99.91% +- 0.31 / lp\n",
      "GRU Avg F1: 88.78% +- 25.2 / lp\n",
      "\n",
      "Number of learning problems:  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 72.95332586733859%, Hard: 81.41845582085365%\n",
      "\n",
      "##LSTM##\n",
      "\n",
      "LSTM Speed: 0.11s +- 0.0 / lp\n",
      "LSTM Avg Acc: 99.89% +- 0.34 / lp\n",
      "LSTM Avg F1: 85.95% +- 23.99 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_vic_shuffle = evaluate_nces(\"vicodi\", [\"SetTransformer\", \"GRU\", \"LSTM\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-replica",
   "metadata": {},
   "source": [
    "### Model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "colonial-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shuffle_examples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "unnecessary-convertible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "NCES evaluation on vicodi KB:\n",
      "##################################################\n",
      "\n",
      "KB namespace:  http://vicodi.org/ontology#\n",
      "\n",
      "Number of learning problems:  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syntactic accuracy, Soft: 88.00231615518238%, Hard: 90.20052529794002%\n",
      "\n",
      "## Ensemble ##\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Object Complement Of not implemented at OWLObjectComplementOf(OWLClass(IRI('http://vicodi.org/ontology#','Illness')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Speed: 0.12s +- 0.0 / lp\n",
      "Ensemble Avg Acc: 99.96% +- 0.15 / lp\n",
      "Ensemble Avg F1: 95.53% +- 14.86 / lp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_vic_ensemble = evaluate_ensemble(\"vicodi\", args, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-sarah",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-pickup",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nces",
   "language": "python",
   "name": "nces"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
